{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "from keras.callbacks import EarlyStopping\n",
    "import soundfile\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 12100731930851906461,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3143997849\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10465110706734145639\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available for tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    '''\n",
    "    Load in the data from file_path\n",
    "    '''\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    MFCC_dataset = []\n",
    "    keys = []\n",
    "    for key in json_data:\n",
    "        keys.append(key)\n",
    "        MFCC_array = np.array(json_data[key])\n",
    "        MFCC_dataset.append(MFCC_array.T)\n",
    "    return MFCC_dataset, keys\n",
    "\n",
    "def get_max_frame_length(MFCC_dataset):\n",
    "    '''\n",
    "    Get the maximum frame length of an MFCC dataset\n",
    "    '''\n",
    "    return max([MFCC.shape[0] for MFCC in MFCC_dataset])\n",
    "\n",
    "def data_length_histogram(MFCC_dataset, cutoff, plot=False):\n",
    "    '''\n",
    "    Filter out datapoints that are either too long or too short\n",
    "    to reduce the size of the dataset when padding is applied\n",
    "    '''\n",
    "    cutoff = cutoff / 100 if cutoff > 1 else cutoff\n",
    "    lengths = [len(mfcc) for mfcc in MFCC_dataset]\n",
    "    binlen = range(max(lengths) + 1)\n",
    "    total = len(lengths)\n",
    "    n_to_remove = int((1 - cutoff) * total)\n",
    "    \n",
    "    if plot:\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.hist(lengths, bins=binlen)\n",
    "    \n",
    "    for _ in range(n_to_remove):\n",
    "        lengths = [len(mfcc) for mfcc in MFCC_dataset]\n",
    "        u_list = np.array(np.unique(np.array(lengths), return_counts=True, return_index=True)).T\n",
    "        u_list = np.array(sorted(u_list, key=lambda x: x[0]))\n",
    "        if u_list[0][2] < u_list[-1][2]:\n",
    "            MFCC_dataset = np.delete(MFCC_dataset, u_list[0][1])\n",
    "        else:\n",
    "            MFCC_dataset = np.delete(MFCC_dataset, u_list[-1][1])\n",
    "    \n",
    "    if plot:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.hist(lengths, bins=binlen)\n",
    "        plt.show()\n",
    "        \n",
    "    return MFCC_dataset\n",
    "\n",
    "def pad_data(MFCC_dataset, number_of_frames):\n",
    "    '''\n",
    "    Pad MFCC_dataset to the specified number of frames\n",
    "    '''\n",
    "    padded_MFCC_dataset = []\n",
    "    original_shapes = []\n",
    "    for MFCC in MFCC_dataset:\n",
    "        original_shapes.append(MFCC.shape)\n",
    "        new_MFCC = np.pad(MFCC, ((number_of_frames-MFCC.shape[0], 0), (0, 0)), 'constant')\n",
    "        padded_MFCC_dataset.append(new_MFCC)\n",
    "    return np.array(padded_MFCC_dataset), np.array(original_shapes)\n",
    "\n",
    "def unpad_data(padded_MFCC_dataset, original_shapes):\n",
    "    '''\n",
    "    Unpad padded MFCC_dataset to the original number of frames\n",
    "    '''\n",
    "    unpadded_MFCCs = []\n",
    "    for i, padded_MFCC in enumerate(padded_MFCC_dataset):\n",
    "        index = padded_MFCC.shape[0] - original_shapes[i][0]\n",
    "        original_MFCC = padded_MFCC[index:]\n",
    "        unpadded_MFCCs.append(original_MFCC)\n",
    "    return unpadded_MFCCs\n",
    "\n",
    "def scale_data(dataset, feature_range=(0, 1)):\n",
    "    '''\n",
    "    Rescale the data to within the specified range\n",
    "    '''\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    scaled_dataset = []\n",
    "    min_values = []\n",
    "    max_values = []\n",
    "    for data in dataset:\n",
    "        scaler.fit(data)\n",
    "        min_values.append(scaler.data_min_)\n",
    "        max_values.append(scaler.data_max_)\n",
    "        scaled_data = scaler.transform(data)\n",
    "        scaled_dataset.append(scaled_data)\n",
    "    return np.array(scaled_dataset), np.array(min_values), np.array(max_values)\n",
    "\n",
    "def unscale_data(scaled_dataset, min_values, max_values, feature_range=(0, 1)):\n",
    "    unscaled_dataset = []\n",
    "    for i, data in enumerate(scaled_dataset):\n",
    "        data_std = (data - feature_range[0]) / (feature_range[1] - feature_range[0])\n",
    "        unscaled_data = data_std * (max_values[i] - min_values[i]) + min_values[i]\n",
    "        unscaled_dataset.append(unscaled_data)\n",
    "    return np.array(unscaled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting data paths\n",
    "train_path = 'Downsampled/nl/MFCC_json_files/MFCC_train.json'\n",
    "test_path = 'Downsampled/nl/MFCC_json_files/MFCC_test.json'\n",
    "validate_path = 'Downsampled/nl/MFCC_json_files/MFCC_validate.json'\n",
    "\n",
    "# Loading the data\n",
    "MFCC_train_set, keys_train_set = load_data(train_path)\n",
    "MFCC_test_set, keys_test_set = load_data(test_path)\n",
    "MFCC_validate_set, keys_validate_set = load_data(validate_path)\n",
    "\n",
    "# Filter data based on length\n",
    "MFCC_train_set = data_length_histogram(MFCC_train_set, 90, plot=False)\n",
    "MFCC_test_set = data_length_histogram(MFCC_test_set, 90, plot=False)\n",
    "MFCC_validate_set = data_length_histogram(MFCC_validate_set, 90, plot=False)\n",
    "\n",
    "# Scaling the data\n",
    "scaled_MFCC_train_set, min_values_train, max_values_train = scale_data(MFCC_train_set)\n",
    "scaled_MFCC_test_set, min_values_test, max_values_test = scale_data(MFCC_test_set)\n",
    "scaled_MFCC_validate_set, min_values_validate, max_values_validate = scale_data(MFCC_validate_set)\n",
    "\n",
    "# Padding the data\n",
    "max_frames_train = get_max_frame_length(scaled_MFCC_train_set)\n",
    "max_frames_test = get_max_frame_length(scaled_MFCC_test_set)\n",
    "max_frames_validate = get_max_frame_length(scaled_MFCC_validate_set)\n",
    "\n",
    "new_number_of_frames = max([max_frames_train, max_frames_test, max_frames_validate])\n",
    "\n",
    "prepped_MFCC_train_set, _ = pad_data(scaled_MFCC_train_set, new_number_of_frames)\n",
    "prepped_MFCC_test_set, _ = pad_data(scaled_MFCC_test_set, new_number_of_frames)\n",
    "prepped_MFCC_validate_set, _ = pad_data(scaled_MFCC_validate_set, new_number_of_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "latent_dim = 500\n",
    "input_dim = 12\n",
    "timesteps = new_number_of_frames\n",
    "temp_dim = round(((input_dim * timesteps) + latent_dim) / 2)\n",
    "\n",
    "# Making the model\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "encoded = LSTM(temp_dim, return_sequences=True, activation='sigmoid')(inputs)\n",
    "encoded = LSTM(latent_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(temp_dim, return_sequences=True, activation='sigmoid')(decoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True, activation='sigmoid')(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3069 samples, validate on 658 samples\n",
      "Epoch 1/10\n",
      "3069/3069 [==============================] - 527s 172ms/step - loss: 0.0905 - val_loss: 0.0859\n",
      "Epoch 2/10\n",
      "3069/3069 [==============================] - 516s 168ms/step - loss: 0.0718 - val_loss: 0.0667\n",
      "Epoch 3/10\n",
      "3069/3069 [==============================] - 534s 174ms/step - loss: 0.0720 - val_loss: 0.0641\n",
      "Epoch 4/10\n",
      "3069/3069 [==============================] - 526s 171ms/step - loss: 0.0749 - val_loss: 0.0692\n",
      "Epoch 5/10\n",
      "3069/3069 [==============================] - 530s 173ms/step - loss: 0.0780 - val_loss: 0.0739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x23e6e733eb8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set callbacks\n",
    "callbacks = [EarlyStopping(patience=2, mode=\"min\", restore_best_weights=True),]\n",
    "\n",
    "# Training the model\n",
    "sequence_autoencoder.fit(prepped_MFCC_train_set,\n",
    "                         prepped_MFCC_train_set,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=(prepped_MFCC_validate_set, prepped_MFCC_validate_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_autoencoder.save('nl_2lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqseq = load_model('Modellen Remco/nl_2lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "658/658 [==============================] - 46s 70ms/step\n",
      "test loss, test acc: 0.06167837891354025\n"
     ]
    }
   ],
   "source": [
    "print('\\n# Evaluate on test data')\n",
    "results = seqseq.evaluate(prepped_MFCC_test_set, prepped_MFCC_test_set, batch_size=10)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sequence_autoencoder.predict(scaled_MFCC_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_MFCC = unscale_data(prediction, min_values_test, max_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19509909\n"
     ]
    }
   ],
   "source": [
    "print(keys_test_set[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_signal = librosa.feature.inverse.mfcc_to_audio(reconstructed_MFCC[200].T)\n",
    "soundfile.write('test_sound.wav', wav_signal, 22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
