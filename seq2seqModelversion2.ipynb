{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, RepeatVector\n",
    "import soundfile\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Downsampled/nl/MFCC_json_files/MFCC_train.json'\n",
    "test_path = 'Downsampled/nl/MFCC_json_files/MFCC_test.json'\n",
    "validate_path = 'Downsampled/nl/MFCC_json_files/MFCC_validate.json'\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    MFCC_dataset = []\n",
    "    keys = []\n",
    "    for key in json_data:\n",
    "        keys.append(key)\n",
    "        MFCC_array = np.array(json_data[key])\n",
    "        MFCC_dataset.append(MFCC_array.T)\n",
    "    return MFCC_dataset, keys\n",
    "\n",
    "def get_max_frame_length(MFCC_dataset):\n",
    "    return max([MFCC.shape[0] for MFCC in MFCC_dataset])\n",
    "\n",
    "def data_length_histogram(MFCC_dataset, cutoff, plot=False):\n",
    "    cutoff = cutoff/100 if cutoff > 1 else cutoff\n",
    "    lengths = [len(mfcc) for mfcc in MFCC_dataset]\n",
    "    binlen = range(max(lengths)+1)\n",
    "    total = len(lengths)\n",
    "    n_to_remove = int((1 - cutoff) * total)\n",
    "    \n",
    "    if plot:\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.hist(lengths, bins=binlen)\n",
    "    \n",
    "    for _ in range(n_to_remove):\n",
    "        lengths = [len(mfcc) for mfcc in MFCC_dataset]\n",
    "        u_list = np.array(np.unique(np.array(lengths), return_counts=True, return_index=True)).T\n",
    "        u_list = np.array(sorted(u_list, key=lambda x: x[0]))\n",
    "        if u_list[0][2] < u_list[-1][2]:\n",
    "            MFCC_dataset = np.delete(MFCC_dataset, u_list[0][1])\n",
    "        else:\n",
    "            MFCC_dataset = np.delete(MFCC_dataset, u_list[-1][1])\n",
    "    \n",
    "    if plot:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.hist(lengths, bins=binlen)\n",
    "        plt.show()\n",
    "        \n",
    "    return MFCC_dataset\n",
    "\n",
    "def pad_data(MFCC_dataset, number_of_frames):\n",
    "    padded_MFCC_dataset = []\n",
    "    for MFCC in MFCC_dataset:\n",
    "        new_MFCC = np.pad(MFCC, ((number_of_frames-MFCC.shape[0], 0), (0, 0)), 'constant')\n",
    "        padded_MFCC_dataset.append(new_MFCC)\n",
    "    return np.array(padded_MFCC_dataset)\n",
    "\n",
    "MFCC_train_set, keys_train_set = load_data(train_path)\n",
    "MFCC_test_set, keys_test_set = load_data(test_path)\n",
    "MFCC_validate_set, keys_validate_set = load_data(validate_path)\n",
    "\n",
    "MFCC_train_set = data_length_histogram(MFCC_train_set, 90, plot=False)\n",
    "MFCC_test_set = data_length_histogram(MFCC_test_set, 90, plot=False)\n",
    "MFCC_validate_set = data_length_histogram(MFCC_validate_set, 90, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames_train = get_max_frame_length(MFCC_train_set)\n",
    "max_frames_test = get_max_frame_length(MFCC_test_set)\n",
    "max_frames_validate = get_max_frame_length(MFCC_validate_set)\n",
    "\n",
    "new_number_of_frames = max([max_frames_train, max_frames_test, max_frames_validate])\n",
    "\n",
    "padded_MFCC_train_set = pad_data(MFCC_train_set, new_number_of_frames)\n",
    "padded_MFCC_test_set = pad_data(MFCC_test_set, new_number_of_frames)\n",
    "padded_MFCC_validate_set = pad_data(MFCC_validate_set, new_number_of_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(dataset, feature_range=(-1, 1)):\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    scaled_dataset = []\n",
    "    min_values = []\n",
    "    max_values = []\n",
    "    for data in dataset:\n",
    "        scaler.fit(data)\n",
    "        min_values.append(scaler.data_min_)\n",
    "        max_values.append(scaler.data_max_)\n",
    "        scaled_data = scaler.transform(data)\n",
    "        scaled_dataset.append(scaled_data)\n",
    "    return np.array(scaled_dataset), np.array(min_values), np.array(max_values)\n",
    "\n",
    "def unscale_data(scaled_dataset, min_values, max_values, feature_range=(-1, 1)):\n",
    "    unscaled_dataset = []\n",
    "    for i, data in enumerate(scaled_dataset):\n",
    "        data_std = (data - feature_range[0]) / (feature_range[1] - feature_range[0])\n",
    "        unscaled_data = data_std * (max_values[i] - min_values[i]) + min_values[i]\n",
    "        unscaled_dataset.append(unscaled_data)\n",
    "    return np.array(unscaled_dataset)\n",
    "\n",
    "scaled_MFCC_train_set, min_values_train, max_values_train = scale_data(padded_MFCC_train_set)\n",
    "scaled_MFCC_test_set, min_values_test, max_values_test = scale_data(padded_MFCC_test_set)\n",
    "scaled_MFCC_validate_set, min_values_validate, max_values_validate = scale_data(padded_MFCC_validate_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epochs = 40\n",
    "latent_dim = 500\n",
    "input_dim = 12\n",
    "timesteps = new_number_of_frames\n",
    "\n",
    "inputs = Input(shape=(timesteps, input_dim))\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "sequence_autoencoder.compile(optimizer='rmsprop', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2873 samples, validate on 411 samples\n",
      "Epoch 1/40\n",
      "2873/2873 [==============================] - 441s 154ms/step - loss: 0.5248 - val_loss: 0.5186\n",
      "Epoch 2/40\n",
      " 940/2873 [========>.....................] - ETA: 4:44 - loss: 0.5209"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-17159d32d42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                          validation_data=(scaled_MFCC_validate_set, scaled_MFCC_validate_set))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sequence_autoencoder.fit(scaled_MFCC_train_set,\n",
    "                         scaled_MFCC_train_set,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=epochs,\n",
    "                         validation_data=(scaled_MFCC_validate_set, scaled_MFCC_validate_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sequence_autoencoder.predict(scaled_MFCC_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_MFCC = unscale_data(prediction, min_values_test, max_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19509909\n"
     ]
    }
   ],
   "source": [
    "print(keys_test_set[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_signal = librosa.feature.inverse.mfcc_to_audio(reconstructed_MFCC[200].T)\n",
    "soundfile.write('test_sound.wav', wav_signal, 22050)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
